{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8cdfcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Google Generative AI directly (no LlamaIndex needed)\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41fcbea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import setup\n",
    "setup.init_django()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4562987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from blog.models import BlogPost, EMBEDDING_LENGTH\n",
    "from blog import services\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fa9da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = BlogPost.objects.filter(can_delete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d74a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL = config('LLM_MODEL', default='gemini-1.5-flash')\n",
    "EMBEDDING_LENGTH = config(\"EMBEDDING_LENGTH\", default=768, cast=int)\n",
    "EMBEDDING_MODEL = config(\"EMBEDDING_MODEL\", default='embedding-001')\n",
    "GEMINI_API_KEY = config(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09762e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raghu\\AppData\\Local\\Temp\\ipykernel_14600\\2678343577.py:1: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  llm = Gemini(\n"
     ]
    }
   ],
   "source": [
    "# Configure Gemini\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Create the model\n",
    "model = genai.GenerativeModel(LLM_MODEL)\n",
    "print(f\"Using model: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6b9d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "response = model.generate_content(\"Hello! Can you tell me about yourself?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b38ef4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple query function using Gemini directly\n",
    "def query_blog_posts(question):\n",
    "    # Get blog posts\n",
    "    posts = BlogPost.objects.all()[:5]\n",
    "    \n",
    "    # Create context\n",
    "    context = \"\\n\\n\".join([f\"Post: {post.title}\\n{post.content[:300]}...\" for post in posts])\n",
    "    \n",
    "    # Generate response\n",
    "    prompt = f\"Based on these blog posts, answer: {question}\\n\\nPosts:\\n{context}\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Test it\n",
    "answer = query_blog_posts(\"What are the main topics?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82ebd282",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db_name = \"vector_db\"\n",
    "vector_db_table_name = \"blogpost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b1a87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_URL = config(\"DATABASE_URL_POOL\")\n",
    "if DATABASE_URL.startswith(\"postgres://\"):\n",
    "    DATABASE_URL = DATABASE_URL.replace(\"postgres://\", \"postgresql://\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a745397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database vector_db already exists\n"
     ]
    }
   ],
   "source": [
    "# create a new Database:\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(DATABASE_URL, isolation_level='AUTOCOMMIT')\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT 1 FROM pg_database WHERE datname = :db_name\"), {\"db_name\":vector_db_name})\n",
    "    db_exists = result.scalar() == 1\n",
    "    \n",
    "    if not db_exists:\n",
    "        # Create the database\n",
    "        connection.execute(text(f\"CREATE DATABASE {vector_db_name}\"))\n",
    "        print(f\"Created database: {vector_db_name}\")\n",
    "    else:\n",
    "        print(f\"Database {vector_db_name} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "420a722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blog post analysis with Gemini\n",
    "if qs.exists():\n",
    "    # Get the first few posts\n",
    "    posts = qs[:3]\n",
    "    \n",
    "    # Create a prompt with the posts\n",
    "    prompt = f\"\"\"\n",
    "    Based on these blog posts, provide a summary of the main topics discussed:\n",
    "    \n",
    "    {chr(10).join([f'Post {i+1}: {post.title}\\n{post.content[:300]}...' for i, post in enumerate(posts)])}\n",
    "    \n",
    "    Please provide a concise summary of the main topics and themes.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    print(\"Blog Analysis:\")\n",
    "    print(response.text)\n",
    "else:\n",
    "    print(\"No blog posts found with can_delete=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine search with Gemini for Q&A\n",
    "def ask_about_blog_posts(question):\n",
    "    \"\"\"\n",
    "    Search for relevant blog posts and use Gemini to answer questions about them\n",
    "    \"\"\"\n",
    "    # Search for relevant posts\n",
    "    search_results = services.search_posts(question, limit=3)\n",
    "    \n",
    "    if not search_results:\n",
    "        return \"No relevant blog posts found.\"\n",
    "    \n",
    "    # Create context from search results\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Post: {result.title}\\n{result.content[:500]}...\" \n",
    "        for result in search_results\n",
    "    ])\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following blog posts, answer this question: {question}\n",
    "    \n",
    "    Blog posts:\n",
    "    {context}\n",
    "    \n",
    "    Please provide a comprehensive answer based on the content above.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Test the Q&A function\n",
    "if qs.exists():\n",
    "    question = \"What are the main topics discussed in the blog posts?\"\n",
    "    answer = ask_about_blog_posts(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple query function using Gemini directly\n",
    "def query_blog_posts(question):\n",
    "    # Get blog posts\n",
    "    posts = BlogPost.objects.all()[:5]\n",
    "    \n",
    "    # Create context\n",
    "    context = \"\\n\\n\".join([f\"Post: {post.title}\\n{post.content[:300]}...\" for post in posts])\n",
    "    \n",
    "    # Generate response\n",
    "    prompt = f\"Based on these blog posts, answer: {question}\\n\\nPosts:\\n{context}\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Test it\n",
    "answer = query_blog_posts(\"What are the main topics?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836a0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talk-to-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
